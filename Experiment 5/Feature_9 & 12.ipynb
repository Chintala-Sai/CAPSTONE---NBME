{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5d092f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "import spacy\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd0b7ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(r\"C:\\Users\\sai\\Desktop\\PRAXIS\\Term - 3\\CAPP\\Datasets\\train.csv\")\n",
    "features = pd.read_csv(r\"C:\\Users\\sai\\Desktop\\PRAXIS\\Term - 3\\CAPP\\Datasets\\features.csv\")\n",
    "patient_notes = pd.read_csv(r\"C:\\Users\\sai\\Desktop\\PRAXIS\\Term - 3\\CAPP\\Datasets\\patient_notes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41c3f58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 603.11it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 770.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "config_feature9.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config_feature9.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 12:58:16.867099: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-03-31 12:58:16.867358: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Saving to output directory: output9\n",
      "[i] Using CPU\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[+] Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[i] Pipeline: ['tok2vec', 'ner']\n",
      "[i] Initial learn rate: 0.001\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     48.67    0.00    0.00    0.00    0.00\n",
      "  2     200      10077.00   2710.60   56.60   60.00   53.57    0.57\n",
      "  5     400         83.95    159.30   68.97   66.67   71.43    0.69\n",
      "  7     600        122.35     95.41   75.00   75.00   75.00    0.75\n",
      " 10     800         42.65     42.89   77.78   80.77   75.00    0.78\n",
      " 12    1000        143.98     35.41   77.78   80.77   75.00    0.78\n",
      " 15    1200         35.32     20.19   69.09   70.37   67.86    0.69\n",
      " 17    1400         41.87     22.39   74.07   76.92   71.43    0.74\n",
      " 20    1600         12.05      4.85   74.07   76.92   71.43    0.74\n",
      " 22    1800          0.06      0.02   76.36   77.78   75.00    0.76\n",
      " 25    2000         76.50     31.91   76.36   77.78   75.00    0.76\n",
      " 27    2200         21.12     12.11   70.37   73.08   67.86    0.70\n",
      " 30    2400         34.38     14.59   77.78   80.77   75.00    0.78\n",
      "[+] Saved pipeline to output directory\n",
      "output9\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 12:58:21.003960: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-03-31 12:58:21.004015: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[2022-03-31 12:58:23,545] [INFO] Set up nlp object from config\n",
      "[2022-03-31 12:58:23,554] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2022-03-31 12:58:23,558] [INFO] Created vocabulary\n",
      "[2022-03-31 12:58:23,559] [INFO] Finished initializing nlp object\n",
      "[2022-03-31 12:58:24,094] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n"
     ]
    }
   ],
   "source": [
    "new_train = train_data[(train_data.feature_num == 9)][['pn_num','annotation',\"location\",\"feature_num\"]]\n",
    "new_train = new_train[new_train.annotation != '[]']\n",
    "\n",
    "new_train[\"annotation\"] = new_train.annotation.str.replace('[','')\n",
    "new_train[\"annotation\"] = new_train.annotation.str.replace(']','')\n",
    "new_train[\"location\"] = new_train.location.str.replace('[','')\n",
    "new_train[\"location\"] = new_train.location.str.replace(']','')\n",
    "\n",
    "new_train = (new_train.set_index(['pn_num',\"feature_num\"]) \n",
    "   .apply(lambda col: col.str.split(',').explode())\n",
    "   .reset_index()\n",
    "   .reindex(new_train.columns, axis=1))\n",
    "\n",
    "new_train[\"start_location\"] = new_train[\"location\"].apply(lambda x: x.split()[0][1:])\n",
    "new_train[\"end_location\"] = new_train[\"location\"].apply(lambda x: x.split()[-1][:-1])\n",
    "\n",
    "row_index = []\n",
    "start_location = []\n",
    "end_location = []\n",
    "pn_notes = set()\n",
    "drop_index = []\n",
    "for index,row in new_train.iterrows():   # Iterating every row in trainn data\n",
    "    if row[\"pn_num\"] not in pn_notes:    # Verifying whether there is a change in pn_num to reinitialize start and end location lists\n",
    "        start_location = []\n",
    "        end_location = []\n",
    "        pn_notes.add(row[\"pn_num\"])      # Appending pn_num\n",
    "        if (row[\"start_location\"] in start_location) | (row[\"end_location\"] in end_location):\n",
    "            row_index.append(index)\n",
    "        start_location.append(row[\"start_location\"])\n",
    "        end_location.append(row[\"end_location\"])\n",
    "    else: \n",
    "        if (row[\"start_location\"] in start_location) | (row[\"end_location\"] in end_location):\n",
    "            row_index.append(index)\n",
    "            if row[\"start_location\"] in start_location:\n",
    "                if row[\"end_location\"] >= new_train.iloc[index -1,5]:\n",
    "                    drop_index.append(index -1)\n",
    "                else:\n",
    "                    drop_index.append(index)\n",
    "            elif row[\"end_location\"] in end_location:\n",
    "                if row[\"start_location\"] <= new_train.iloc[index -1,4]:\n",
    "                    drop_index.append(index - 1)\n",
    "                else:\n",
    "                    drop_index.append(index)\n",
    "        start_location.append(row[\"start_location\"])\n",
    "        end_location.append(row[\"end_location\"])\n",
    "        \n",
    "new_train = new_train.drop(drop_index,axis=0)\n",
    "\n",
    "pn_Num_list = list(new_train.pn_num.unique())\n",
    "\n",
    "final_train = []   # appending all the individual rows\n",
    "for patient_number in pn_Num_list:  # for each unique patient number\n",
    "    entities = []  # saving individaual entities locations\n",
    "    for row_index,row in new_train[new_train.pn_num == patient_number].iterrows():  # for each unique patient number \n",
    "        \n",
    "        entities.append((int(row[\"start_location\"]),int(row[\"end_location\"]),\"Feature_\"+ str(row[\"feature_num\"]))) # store the locations in numeric format\n",
    "        \n",
    "    text = patient_notes[patient_notes.pn_num == patient_number][\"pn_history\"].values[0] # save the text for the unique patient number\n",
    "    \n",
    "    final_train.append((text,{\"entities\":entities}))  # for each unique pn_number append to final list\n",
    "    \n",
    "    \n",
    "# Serializing json \n",
    "json_object = json.dumps(final_train, indent = 4)\n",
    "  \n",
    "# Writing to sample.json\n",
    "with open(\"sample_feature9.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)\n",
    "    \n",
    "import json\n",
    " \n",
    "# Opening JSON file\n",
    "f = open(\"sample_feature9.json\")\n",
    " \n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "train = data[:int(len(data)*0.8)]\n",
    "validation = data[int(len(data)*0.8):]\n",
    "\n",
    "# converting the json file to spacy file in order to use it in training model\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "def create_training(TRAIN_DATA):\n",
    "    db = DocBin()\n",
    "    for text, annot in tqdm(TRAIN_DATA):\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print (\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    return (db)\n",
    "\n",
    "#train data set\n",
    "\n",
    "camp_train = create_training(train)\n",
    "camp_train.to_disk(\"nbme_train_feature9.spacy\")\n",
    "\n",
    "camp_train = create_training(validation)\n",
    "camp_train.to_disk(\"nbme_validation_feature9.spacy\")\n",
    "\n",
    "!python -m spacy init fill-config ./base_config_feature9.cfg ./config_feature9.cfg\n",
    "\n",
    "!python -m spacy train config_feature9.cfg --output ./output9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b88ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 75/75 [00:00<00:00, 477.91it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 653.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "config_feature12.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config_feature12.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 13:00:52.648006: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-03-31 13:00:52.648048: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Saving to output directory: output12\n",
      "[i] Using CPU\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[+] Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[i] Pipeline: ['tok2vec', 'ner']\n",
      "[i] Initial learn rate: 0.001\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     74.33    0.00    0.00    0.00    0.00\n",
      "  2     200          4.82   1289.95   97.30  100.00   94.74    0.97\n",
      "  5     400          0.00      0.00   94.44  100.00   89.47    0.94\n",
      "  8     600          0.00      0.00   94.44  100.00   89.47    0.94\n",
      " 10     800          0.00      0.00   94.44  100.00   89.47    0.94\n",
      " 13    1000          0.00      0.00   94.44  100.00   89.47    0.94\n",
      " 16    1200          0.00      0.00   94.44  100.00   89.47    0.94\n",
      " 18    1400          0.00      0.00   94.44  100.00   89.47    0.94\n",
      " 21    1600          0.00      0.00   94.44  100.00   89.47    0.94\n",
      " 24    1800          0.00      0.00   94.44  100.00   89.47    0.94\n",
      "[+] Saved pipeline to output directory\n",
      "output12\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 13:00:56.692185: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-03-31 13:00:56.692296: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[2022-03-31 13:00:59,271] [INFO] Set up nlp object from config\n",
      "[2022-03-31 13:00:59,278] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2022-03-31 13:00:59,281] [INFO] Created vocabulary\n",
      "[2022-03-31 13:00:59,282] [INFO] Finished initializing nlp object\n",
      "[2022-03-31 13:00:59,713] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n"
     ]
    }
   ],
   "source": [
    "new_train = train_data[(train_data.feature_num == 12)][['pn_num','annotation',\"location\",\"feature_num\"]]\n",
    "new_train = new_train[new_train.annotation != '[]']\n",
    "\n",
    "new_train[\"annotation\"] = new_train.annotation.str.replace('[','')\n",
    "new_train[\"annotation\"] = new_train.annotation.str.replace(']','')\n",
    "new_train[\"location\"] = new_train.location.str.replace('[','')\n",
    "new_train[\"location\"] = new_train.location.str.replace(']','')\n",
    "\n",
    "new_train = (new_train.set_index(['pn_num',\"feature_num\"]) \n",
    "   .apply(lambda col: col.str.split(',').explode())\n",
    "   .reset_index()\n",
    "   .reindex(new_train.columns, axis=1))\n",
    "\n",
    "new_train[\"start_location\"] = new_train[\"location\"].apply(lambda x: x.split()[0][1:])\n",
    "new_train[\"end_location\"] = new_train[\"location\"].apply(lambda x: x.split()[-1][:-1])\n",
    "\n",
    "row_index = []\n",
    "start_location = []\n",
    "end_location = []\n",
    "pn_notes = set()\n",
    "drop_index = []\n",
    "for index,row in new_train.iterrows():   # Iterating every row in trainn data\n",
    "    if row[\"pn_num\"] not in pn_notes:    # Verifying whether there is a change in pn_num to reinitialize start and end location lists\n",
    "        start_location = []\n",
    "        end_location = []\n",
    "        pn_notes.add(row[\"pn_num\"])      # Appending pn_num\n",
    "        if (row[\"start_location\"] in start_location) | (row[\"end_location\"] in end_location):\n",
    "            row_index.append(index)\n",
    "        start_location.append(row[\"start_location\"])\n",
    "        end_location.append(row[\"end_location\"])\n",
    "    else: \n",
    "        if (row[\"start_location\"] in start_location) | (row[\"end_location\"] in end_location):\n",
    "            row_index.append(index)\n",
    "            if row[\"start_location\"] in start_location:\n",
    "                if row[\"end_location\"] >= new_train.iloc[index -1,5]:\n",
    "                    drop_index.append(index -1)\n",
    "                else:\n",
    "                    drop_index.append(index)\n",
    "            elif row[\"end_location\"] in end_location:\n",
    "                if row[\"start_location\"] <= new_train.iloc[index -1,4]:\n",
    "                    drop_index.append(index - 1)\n",
    "                else:\n",
    "                    drop_index.append(index)\n",
    "        start_location.append(row[\"start_location\"])\n",
    "        end_location.append(row[\"end_location\"])\n",
    "        \n",
    "new_train = new_train.drop(drop_index,axis=0)\n",
    "\n",
    "pn_Num_list = list(new_train.pn_num.unique())\n",
    "\n",
    "final_train = []   # appending all the individual rows\n",
    "for patient_number in pn_Num_list:  # for each unique patient number\n",
    "    entities = []  # saving individaual entities locations\n",
    "    for row_index,row in new_train[new_train.pn_num == patient_number].iterrows():  # for each unique patient number \n",
    "        \n",
    "        entities.append((int(row[\"start_location\"]),int(row[\"end_location\"]),\"Feature_\"+ str(row[\"feature_num\"]))) # store the locations in numeric format\n",
    "        \n",
    "    text = patient_notes[patient_notes.pn_num == patient_number][\"pn_history\"].values[0] # save the text for the unique patient number\n",
    "    \n",
    "    final_train.append((text,{\"entities\":entities}))  # for each unique pn_number append to final list\n",
    "    \n",
    "    \n",
    "# Serializing json \n",
    "json_object = json.dumps(final_train, indent = 4)\n",
    "  \n",
    "# Writing to sample.json\n",
    "with open(\"sample_feature12.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)\n",
    "    \n",
    "import json\n",
    " \n",
    "# Opening JSON file\n",
    "f = open(\"sample_feature12.json\")\n",
    " \n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "train = data[:int(len(data)*0.8)]\n",
    "validation = data[int(len(data)*0.8):]\n",
    "\n",
    "# converting the json file to spacy file in order to use it in training model\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "def create_training(TRAIN_DATA):\n",
    "    db = DocBin()\n",
    "    for text, annot in tqdm(TRAIN_DATA):\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print (\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    return (db)\n",
    "\n",
    "#train data set\n",
    "\n",
    "camp_train = create_training(train)\n",
    "camp_train.to_disk(\"nbme_train_feature12.spacy\")\n",
    "\n",
    "camp_train = create_training(validation)\n",
    "camp_train.to_disk(\"nbme_validation_feature12.spacy\")\n",
    "\n",
    "!python -m spacy init fill-config ./base_config_feature12.cfg ./config_feature12.cfg\n",
    "\n",
    "!python -m spacy train config_feature12.cfg --output ./output12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2561e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
