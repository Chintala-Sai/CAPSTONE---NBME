{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "\n",
    "\n",
    "# importing the nltk suite \n",
    "import nltk\n",
    "  \n",
    "# importing jaccard distance\n",
    "# and ngrams from nltk.util\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "from nltk.metrics import edit_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Manoj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Downloading and importing\n",
    "# package 'words' from nltk corpus\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "  \n",
    "  \n",
    "correct_words = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adrenalin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# list of incorrect spellings\n",
    "# that need to be corrected \n",
    "incorrect_words=['Adreenaline']\n",
    "  \n",
    "# loop for finding correct spellings\n",
    "# based on jaccard distance\n",
    "# and printing the correct word\n",
    "for word in incorrect_words:\n",
    "    temp = [(jaccard_distance(set(ngrams(word, 2)),\n",
    "                              set(ngrams(w, 2))),w)\n",
    "            for w in correct_words if w[0]==word[0]]\n",
    "    print(sorted(temp, key = lambda val:val[0])[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "spell = Speller(lang='en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"C:\\Users\\Manoj\\praxis Term 3\\Cap Stone\\capstone project\\train.csv\")\n",
    "features = pd.read_csv(r\"C:\\Users\\Manoj\\praxis Term 3\\Cap Stone\\capstone project\\features.csv\")\n",
    "patient_notes = pd.read_csv(r\"C:\\Users\\Manoj\\praxis Term 3\\Cap Stone\\capstone project\\patient_notes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check this\n",
    "training = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "import spacy\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For feature 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = training[(training.feature_num == 4)][['pn_num','annotation',\"location\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = new_train[new_train.annotation != '[]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train[\"annotation\"] = new_train.annotation.str.replace('[','')\n",
    "new_train[\"annotation\"] = new_train.annotation.str.replace(']','')\n",
    "new_train[\"location\"] = new_train.location.str.replace('[','')\n",
    "new_train[\"location\"] = new_train.location.str.replace(']','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.loc[new_train.location == \"'883 900'\",'annotation'] = \"['dad  heart attack']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = (new_train.set_index(['pn_num']) \n",
    "   .apply(lambda col: col.str.split(',').explode())\n",
    "   .reset_index()\n",
    "   .reindex(new_train.columns, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train[\"start_location\"] = new_train[\"location\"].apply(lambda x: x.split()[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train[\"end_location\"] = new_train[\"location\"].apply(lambda x: x.split()[-1][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_train[\"annotation\"] = new_train[\"annotation\"].apply(lambda x: spell(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intersecting = set(patient_notes.pn_num).intersection(new_train.pn_num)\n",
    "#for i in intersecting:\n",
    "#    for row_index,row in new_train[new_train.pn_num == i].iterrows():  # for each unique patient number \n",
    "#        new_train.loc[row_index,\"patientnotes\"] = patient_notes.loc[i,\"pn_history\"]\n",
    "#new_train[\"patientnotes\"] = new_train[\"patientnotes\"].apply(lambda x: spell(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn_Num_list = list(new_train.pn_num.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pn_Num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = []   # appending all the individual rows\n",
    "for patient_number in pn_Num_list:  # for each unique patient number\n",
    "    entities = []  # saving individaual entities locations\n",
    "    for row_index,row in new_train[new_train.pn_num == patient_number].iterrows():  # for each unique patient number \n",
    "        \n",
    "        entities.append((int(row[\"start_location\"]),int(row[\"end_location\"]),\"Feature\")) # store the locations in numeric format\n",
    "        \n",
    "    text = patient_notes[patient_notes.pn_num == patient_number][\"pn_history\"].values[0] # save the text for the unique patient number\n",
    "    \n",
    "    final_train.append((text,{\"entities\":entities}))  # for each unique pn_number append to final list\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = final_train[:40]\n",
    "validation = final_train[40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the json file to spacy file in order to use it in training model\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "def create_training(TRAIN_DATA):\n",
    "    db = DocBin()\n",
    "    for text, annot in tqdm(TRAIN_DATA):\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print (\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    return (db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 365.09it/s]\n"
     ]
    }
   ],
   "source": [
    "#train data set\n",
    "\n",
    "camp_train = create_training(train)\n",
    "camp_train.to_disk(\"nbme_train.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 623.85it/s]\n"
     ]
    }
   ],
   "source": [
    "#test data set\n",
    "\n",
    "camp_train = create_training(validation)\n",
    "camp_train.to_disk(\"nbme_validation.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 10:38:39.722937: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-04-03 10:38:39.722979: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy init fill-config ./base_config.cfg ./config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Saving to output directory: output\n",
      "[i] Using CPU\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[+] Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[i] Pipeline: ['tok2vec', 'ner']\n",
      "[i] Initial learn rate: 0.001\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     70.33    0.00    0.00    0.00    0.00\n",
      "  5     200         23.91   1280.55   66.67   75.00   60.00    0.67\n",
      " 10     400        794.48     77.28   70.00   70.00   70.00    0.70\n",
      " 15     600          0.12      0.18   70.00   70.00   70.00    0.70\n",
      " 20     800          0.00      0.00   70.00   70.00   70.00    0.70\n",
      " 25    1000          0.00      0.00   70.00   70.00   70.00    0.70\n",
      " 30    1200          0.00      0.00   70.00   70.00   70.00    0.70\n",
      " 35    1400          0.00      0.00   63.16   66.67   60.00    0.63\n",
      " 40    1600          0.00      0.00   63.16   66.67   60.00    0.63\n",
      " 45    1800          0.00      0.00   63.16   66.67   60.00    0.63\n",
      " 50    2000          0.00      0.00   70.00   70.00   70.00    0.70\n",
      "[+] Saved pipeline to output directory\n",
      "output\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 10:38:56.468185: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-04-03 10:38:56.468234: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[2022-04-03 10:38:59,139] [INFO] Set up nlp object from config\n",
      "[2022-04-03 10:38:59,139] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2022-04-03 10:38:59,154] [INFO] Created vocabulary\n",
      "[2022-04-03 10:38:59,154] [INFO] Finished initializing nlp object\n",
      "[2022-04-03 10:38:59,495] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy train config.cfg --output ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Saving to output directory: output\n",
      "[i] Using CPU\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[+] Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[i] Pipeline: ['tok2vec', 'ner']\n",
      "[i] Initial learn rate: 0.001\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     70.33    0.00    0.00    0.00    0.00\n",
      "  5     200         23.91   1280.55   66.67   75.00   60.00    0.67\n",
      " 10     400        794.48     77.28   70.00   70.00   70.00    0.70\n",
      " 15     600          0.12      0.18   70.00   70.00   70.00    0.70\n",
      " 20     800          0.00      0.00   70.00   70.00   70.00    0.70\n",
      " 25    1000          0.00      0.00   70.00   70.00   70.00    0.70\n",
      " 30    1200          0.00      0.00   70.00   70.00   70.00    0.70\n",
      " 35    1400          0.00      0.00   63.16   66.67   60.00    0.63\n",
      " 40    1600          0.00      0.00   63.16   66.67   60.00    0.63\n",
      " 45    1800          0.00      0.00   63.16   66.67   60.00    0.63\n",
      " 50    2000          0.00      0.00   70.00   70.00   70.00    0.70\n",
      "[+] Saved pipeline to output directory\n",
      "output\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 14:41:59.660188: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-03-31 14:41:59.660240: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[2022-03-31 14:42:03,016] [INFO] Set up nlp object from config\n",
      "[2022-03-31 14:42:03,028] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2022-03-31 14:42:03,031] [INFO] Created vocabulary\n",
      "[2022-03-31 14:42:03,033] [INFO] Finished initializing nlp object\n",
      "[2022-03-31 14:42:03,449] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy train config.cfg --output ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Feature 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = training[(training.feature_num == 5)][['pn_num','annotation',\"location\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.loc[new_train.location == \"'883 900'\",'annotation'] = \"['dad  heart attack']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = new_train[new_train.annotation != '[]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train[\"annotation\"] = new_train.annotation.str.replace('[','')\n",
    "new_train[\"annotation\"] = new_train.annotation.str.replace(']','')\n",
    "new_train[\"location\"] = new_train.location.str.replace('[','')\n",
    "new_train[\"location\"] = new_train.location.str.replace(']','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = (new_train.set_index(['pn_num']) \n",
    "   .apply(lambda col: col.str.split(',').explode())\n",
    "   .reset_index()\n",
    "   .reindex(new_train.columns, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train[\"start_location\"] = new_train[\"location\"].apply(lambda x: x.split()[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train[\"end_location\"] = new_train[\"location\"].apply(lambda x: x.split()[-1][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pn_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "      <th>start_location</th>\n",
       "      <th>end_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2198</td>\n",
       "      <td>'DENIES TEMPERATURE INTOLERANCE'</td>\n",
       "      <td>'452 458;478 501'</td>\n",
       "      <td>452</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2198</td>\n",
       "      <td>'DENIES SKIN/HAIR CHANGES'</td>\n",
       "      <td>'452 476'</td>\n",
       "      <td>452</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pn_num                        annotation           location  \\\n",
       "45    2198  'DENIES TEMPERATURE INTOLERANCE'  '452 458;478 501'   \n",
       "46    2198        'DENIES SKIN/HAIR CHANGES'          '452 476'   \n",
       "\n",
       "   start_location end_location  \n",
       "45            452          501  \n",
       "46            452          476  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train[new_train.start_location == \"452\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pn_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "      <th>start_location</th>\n",
       "      <th>end_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2198</td>\n",
       "      <td>'DENIES TEMPERATURE INTOLERANCE'</td>\n",
       "      <td>'452 458;478 501'</td>\n",
       "      <td>452</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pn_num                        annotation           location  \\\n",
       "45    2198  'DENIES TEMPERATURE INTOLERANCE'  '452 458;478 501'   \n",
       "\n",
       "   start_location end_location  \n",
       "45            452          501  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train[new_train.end_location == \"501\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = new_train.drop(index=46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = new_train.drop(index=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = new_train.drop(index=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = new_train.drop(index=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = new_train.drop(index=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = new_train.drop(index=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = new_train.drop(index=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = new_train.drop(index=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_train[\"annotation\"] = new_train[\"annotation\"].apply(lambda x: spell(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intersecting = set(patient_notes.pn_num).intersection(new_train.pn_num)\n",
    "#for i in intersecting:\n",
    "#    for row_index,row in new_train[new_train.pn_num == i].iterrows():  # for each unique patient number \n",
    "#        new_train.loc[row_index,\"patientnotes\"] = patient_notes.loc[i,\"pn_history\"]\n",
    "#new_train[\"patientnotes\"] = new_train[\"patientnotes\"].apply(lambda x: spell(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn_Num_list = list(new_train.pn_num.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pn_Num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = []   # appending all the individual rows\n",
    "for patient_number in pn_Num_list:  # for each unique patient number\n",
    "    entities = []  # saving individaual entities locations\n",
    "    for row_index,row in new_train[new_train.pn_num == patient_number].iterrows():  # for each unique patient number \n",
    "        \n",
    "        entities.append((int(row[\"start_location\"]),int(row[\"end_location\"]),\"Feature\")) # store the locations in numeric format\n",
    "        \n",
    "    text = patient_notes[patient_notes.pn_num == patient_number][\"pn_history\"].values[0] # save the text for the unique patient number\n",
    "    \n",
    "    final_train.append((text,{\"entities\":entities}))  # for each unique pn_number append to final list\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = final_train[:33]\n",
    "validation = final_train[33:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the json file to spacy file in order to use it in training model\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "def create_training(TRAIN_DATA):\n",
    "    db = DocBin()\n",
    "    for text, annot in tqdm(TRAIN_DATA):\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print (\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    return (db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 33/33 [00:00<00:00, 510.74it/s]\n"
     ]
    }
   ],
   "source": [
    "#train data set\n",
    "\n",
    "camp_train = create_training(train)\n",
    "camp_train.to_disk(\"nbme_train.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 579.77it/s]\n"
     ]
    }
   ],
   "source": [
    "#test data set\n",
    "\n",
    "camp_train = create_training(validation)\n",
    "camp_train.to_disk(\"nbme_validation.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 10:46:01.729849: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-04-03 10:46:01.729926: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy init fill-config ./base_config.cfg ./config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Saving to output directory: output\n",
      "[i] Using CPU\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 14:48:56.517700: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-03-31 14:48:56.517773: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[2022-03-31 14:49:06,523] [INFO] Set up nlp object from config\n",
      "[2022-03-31 14:49:06,556] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2022-03-31 14:49:06,566] [INFO] Created vocabulary\n",
      "[2022-03-31 14:49:06,568] [INFO] Finished initializing nlp object\n",
      "[2022-03-31 14:49:07,753] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[i] Pipeline: ['tok2vec', 'ner']\n",
      "[i] Initial learn rate: 0.001\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     69.33    0.00    0.00    0.00    0.00\n",
      "  6     200       4035.48   2437.36   14.29   20.00   11.11    0.14\n",
      " 12     400         31.24     75.87   44.44   44.44   44.44    0.44\n",
      " 18     600         20.79     23.06   26.67   33.33   22.22    0.27\n",
      " 24     800         23.13     24.17   28.57   25.00   33.33    0.29\n",
      " 30    1000        280.00     33.58   34.78   28.57   44.44    0.35\n",
      " 36    1200          2.38      1.87   33.33   33.33   33.33    0.33\n",
      " 42    1400         12.22      4.01   42.11   40.00   44.44    0.42\n",
      " 48    1600         85.82     26.59   25.00   28.57   22.22    0.25\n",
      " 54    1800        546.44     22.56   22.22   22.22   22.22    0.22\n",
      " 60    2000         17.52      7.66   38.10   33.33   44.44    0.38\n",
      "[+] Saved pipeline to output directory\n",
      "output\\model-last\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy train config.cfg --output ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
